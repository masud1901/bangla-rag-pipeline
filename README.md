# Multilingual RAG System - AI Engineer (Level-1) Assessment

A Retrieval-Augmented Generation (RAG) system that supports both English and Bengali queries, built with FastAPI, Streamlit, and Docker. This project was developed as part of the AI Engineer (Level-1) Technical Assessment.

## ğŸ¯ Assessment Requirements & Implementation Status

### âœ… **CORE TASK - COMPLETED**

**Requirement**: Build a basic RAG application that accepts user queries in English and Bangla
- âœ… **Implemented**: FastAPI backend with multilingual support
- âœ… **Implemented**: Streamlit GUI with Bengali sample questions
- âœ… **Implemented**: Dual embedding system (Cohere + OpenAI) for better multilingual understanding

**Requirement**: Retrieves relevant document chunks from a small knowledge base
- âœ… **Implemented**: ChromaDB vector database with 1,869 total vectors
- âœ… **Implemented**: Dual collections (Cohere: 903 vectors, OpenAI: 966 vectors)
- âœ… **Implemented**: Semantic similarity search with configurable top-k retrieval

**Requirement**: Generates answers based on the retrieved information
- âœ… **Implemented**: OpenAI GPT-3.5-turbo for answer generation
- âœ… **Implemented**: Cohere rerank for improved relevance
- âœ… **Implemented**: Context-aware prompt engineering

### âœ… **KNOWLEDGE BASE - COMPLETED**

**Requirement**: Use the following Bangla Book - HSC26 Bangla 1st paper
- âœ… **Implemented**: Successfully ingested `Bangla_book.pdf` (19MB)
- âœ… **Implemented**: Extracted 304 pages with OCR support
- âœ… **Implemented**: Created 966 chunks with semantic chunking

**Requirement**: Proper Pre-Processing & data cleaning for better chunk accuracy
- âœ… **Implemented**: Advanced OCR with Tesseract (Bengali + English)
- âœ… **Implemented**: OCR caching system for performance optimization
- âœ… **Implemented**: Text cleaning and normalization
- âœ… **Implemented**: Parallel processing for efficiency

**Requirement**: Document Chunking & Vectorize
- âœ… **Implemented**: RecursiveCharacterTextSplitter with Bengali-aware separators
- âœ… **Implemented**: Dual embedding generation (Cohere + OpenAI)
- âœ… **Implemented**: Semantic chunking strategy
- âœ… **Implemented**: Vector storage in ChromaDB

### âœ… **MEMORY MANAGEMENT - COMPLETED**

**Requirement**: Maintain Long-Short term memory
- âœ… **Short-Term Memory**: Streamlit session state for chat history
- âœ… **Short-Term Memory**: Conversation context preservation
- âœ… **Long-Term Memory**: ChromaDB with persistent storage
- âœ… **Long-Term Memory**: 1,869 total vectors from document corpus

### âŒ **SAMPLE TEST CASES - NEEDS VERIFICATION**

**Assessment Questions** (Need to test):
1. "à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦¸à§à¦ªà§à¦°à§à¦· à¦•à¦¾à¦•à§‡ à¦¬à¦²à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡?" â†’ Expected: "à¦¶à§à¦®à§à¦­à§à¦¨à¦¾à¦¥"
2. "à¦•à¦¾à¦•à§‡ à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦—à§à¦¯ à¦¦à§‡à¦¬à¦¤à¦¾ à¦¬à¦²à§‡ à¦‰à¦²à§à¦²à§‡à¦– à¦•à¦°à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡?" â†’ Expected: "à¦®à¦¾à¦®à¦¾à¦•à§‡"
3. "à¦¬à¦¿à¦¯à¦¼à§‡à¦° à¦¸à¦®à¦¯à¦¼ à¦•à¦²à§à¦¯à¦¾à¦£à§€à¦° à¦ªà§à¦°à¦•à§ƒà¦¤ à¦¬à¦¯à¦¼à¦¸ à¦•à¦¤ à¦›à¦¿à¦²?" â†’ Expected: "à§§à§« à¦¬à¦›à¦°"

**Status**: âŒ **Not Yet Verified** - Need to test these specific questions

### âœ… **BONUS TASKS - COMPLETED**

**Requirement**: Simple Conversation API
- âœ… **Implemented**: FastAPI REST API with multiple endpoints
- âœ… **Implemented**: `/query` endpoint for RAG interactions
- âœ… **Implemented**: `/health` endpoint for system monitoring
- âœ… **Implemented**: `/debug/retrieve` for debugging
- âœ… **Implemented**: Complete API documentation

**Requirement**: RAG Evaluation
- âœ… **Implemented**: Cosine similarity scores in retrieval
- âœ… **Implemented**: Relevance scoring in responses
- âœ… **Implemented**: Debug endpoints for evaluation
- âœ… **Implemented**: System health monitoring

### âœ… **INDUSTRY-STANDARD TOOLS - COMPLETED**

**âœ… Our Tech Stack**:
- **Vector Database**: ChromaDB (industry standard)
- **LLM**: OpenAI GPT-3.5-turbo
- **Embeddings**: Cohere + OpenAI (dual approach)
- **Framework**: FastAPI + Streamlit
- **Containerization**: Docker + Docker Compose
- **Deployment**: AWS EC2 + nginx + Cloudflare

## ğŸš€ Features

- **Multilingual Support**: Handle queries in both English and Bengali
- **Dual Embeddings**: Uses both Cohere and OpenAI embeddings for better retrieval
- **OCR Processing**: Advanced PDF text extraction with OCR support
- **Vector Database**: ChromaDB for efficient similarity search
- **Reranking**: Cohere rerank for improved answer relevance
- **Web Interface**: Streamlit GUI for easy interaction
- **REST API**: FastAPI backend for programmatic access
- **Docker Support**: Complete containerization for easy deployment
- **Production Deployment**: AWS EC2 with nginx reverse proxy
- **Domain Configuration**: Ready for Cloudflare integration

## ğŸ“‹ Prerequisites

- Docker and Docker Compose
- Python 3.9+
- API Keys:
  - Cohere API Key
  - OpenAI API Key
  - Pinecone API Key (optional)

## ğŸ› ï¸ Quick Start

### 1. Clone the Repository
```bash
git clone <your-repo-url>
cd multilingual-rag-system
```

### 2. Set Up Environment Variables
```bash
cp .env.example .env
# Edit .env with your API keys
```

### 3. Add Your PDF Documents
```bash
# Place your PDF files in the data/ directory
mkdir -p data
cp your_document.pdf data/
```

### 4. Start the System
```bash
# Start all services
make up

# Or start individual services
make api    # FastAPI backend only
make gui    # Streamlit GUI only
```

### 5. Ingest Your Documents
```bash
# Ingest a specific PDF
make ingest PDF_NAME=your_document.pdf

# Clear existing data and re-ingest
make ingest-clear PDF_NAME=your_document.pdf
```

### 6. Access the System
- **Web Interface**: http://localhost:8501
- **API Documentation**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health

## ğŸ“ Project Structure

```
â”œâ”€â”€ docker-compose.yml          # Docker services configuration
â”œâ”€â”€ Dockerfile                  # Main application Dockerfile
â”œâ”€â”€ gui/                       # Streamlit GUI
â”‚   â”œâ”€â”€ app.py                 # Main GUI application
â”‚   â”œâ”€â”€ Dockerfile             # GUI Dockerfile
â”‚   â””â”€â”€ requirements.txt       # GUI dependencies
â”œâ”€â”€ src/                       # Core application
â”‚   â”œâ”€â”€ core/                  # Configuration and models
â”‚   â”œâ”€â”€ services/              # Business logic services
â”‚   â”œâ”€â”€ pipeline/              # Data processing pipelines
â”‚   â””â”€â”€ main.py               # FastAPI application
â”œâ”€â”€ data/                      # PDF documents (mounted volume)
â”œâ”€â”€ cache/                     # OCR cache (mounted volume)
â”œâ”€â”€ Makefile                   # Development commands
â””â”€â”€ requirements.txt           # Main application dependencies
```

## ğŸ”§ Configuration

### Environment Variables
- `COHERE_API_KEY`: Your Cohere API key
- `OPENAI_API_KEY`: Your OpenAI API key
- `PINECONE_API_KEY`: Your Pinecone API key (optional)
- `EMBEDDING_PROVIDER`: "cohere", "openai", or "both"
- `TOP_K_RETRIEVAL`: Number of chunks to retrieve (default: 12)
- `RELEVANCE_THRESHOLD`: Minimum relevance score (default: 0.5)

### API Endpoints

#### Health Check
```bash
GET /health
```

#### Query Endpoint
```bash
POST /query
{
  "query": "Your question here",
  "top_k": 12
}
```

#### Debug Retrieval
```bash
POST /debug/retrieve
{
  "query": "Your question here",
  "top_k": 12
}
```

## ğŸ³ Docker Commands

```bash
# Start all services
make up

# Stop all services
make down

# View logs
make logs

# Rebuild images
make build

# Start only API
make api

# Start only GUI
make gui
```

## ğŸ“Š System Status

Check system health and vector database statistics:
```bash
curl http://localhost:8000/health
```

## ğŸ” Troubleshooting

### Common Issues

1. **API Keys Not Set**
   - Ensure all required API keys are in your `.env` file

2. **PDF Processing Issues**
   - Check that PDF files are in the `data/` directory
   - Verify PDF files are not corrupted

3. **Vector Database Issues**
   - Clear and re-ingest data: `make ingest-clear PDF_NAME=your_file.pdf`

4. **OCR Processing Slow**
   - The system uses OCR caching to speed up re-processing
   - Check cache status in the GUI

### Debug Commands

```bash
# Check system health
curl http://localhost:8000/health

# Test API directly
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"query": "Your question here"}'
```

## ğŸ¯ Assessment Questions & Answers

### What method or library did you use to extract the text, and why?
**Answer**: We used PyMuPDF (fitz) for basic text extraction and Tesseract OCR for Bengali text. We chose this combination because:
- PyMuPDF is fast for English text extraction
- Tesseract supports Bengali language with proper training
- OCR caching system prevents re-processing the same pages
- Parallel processing optimizes performance

### What chunking strategy did you choose?
**Answer**: We used RecursiveCharacterTextSplitter with Bengali-aware separators:
- Character limit: 1000 characters per chunk
- Overlap: 200 characters for context continuity
- Bengali separators: `à¥¤`, `?`, `!`, `\n\n`, `\n`, ` `, ``
- This works well for semantic retrieval because it preserves sentence boundaries while maintaining context

### What embedding model did you use?
**Answer**: We implemented a dual embedding approach:
- **Cohere**: `embed-multilingual-v3.0` for multilingual support
- **OpenAI**: `text-embedding-3-small` for high-quality embeddings
- This dual approach captures meaning better by leveraging both models' strengths

### How are you comparing the query with your stored chunks?
**Answer**: We use cosine similarity with ChromaDB:
- Dual embedding search (Cohere + OpenAI)
- Configurable top-k retrieval (default: 12)
- Cohere rerank for improved relevance
- ChromaDB provides efficient similarity search with metadata filtering

### How do you ensure meaningful comparison?
**Answer**: 
- Dual embedding approach captures different semantic aspects
- Reranking improves relevance of retrieved chunks
- Context-aware prompt engineering for answer generation
- For vague queries, the system retrieves multiple chunks and synthesizes an answer

### Do the results seem relevant?
**Answer**: The system shows good technical implementation but needs verification with the specific assessment questions. The dual embedding approach and reranking should provide relevant results, but Bengali OCR quality and chunking strategy may need optimization.

## ğŸš€ Production Deployment

### AWS EC2 Deployment
- âœ… **Deployed**: Complete system on AWS EC2 instance
- âœ… **Configured**: nginx reverse proxy
- âœ… **Secured**: API internal access, GUI public access
- âœ… **Domain Ready**: Configured for `ragbangla.nimbusrb.com`

### Deployment Commands
```bash
# SSH to EC2 instance
ssh -i your-key.pem ec2-user@your-ec2-ip

# Start services
cd /path/to/project
docker-compose up -d

# Check status
docker-compose ps
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ†˜ Support

For issues and questions:
1. Check the troubleshooting section
2. Review the logs: `make logs`
3. Open an issue on GitHub

## âš ï¸ Known Limitations

1. **Bengali OCR Quality**: May need improvement for complex Bengali text
2. **Assessment Question Testing**: Need to verify specific Bengali question accuracy
3. **Chunking Optimization**: May need fine-tuning for Bengali content
4. **Rate Limiting**: Cohere trial tier has rate limits affecting ingestion speed 